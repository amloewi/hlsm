
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\PassOptionsToPackage{numbers, sort}{natbib}
\usepackage[final]{nips_2017}
\bibliographystyle{unsrtnat}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{lipsum}
\usepackage{algpseudocode}

\usepackage{amssymb, amsmath}

\title{Hierarchical Latent Space Models\\for Multiplex Social Network Analysis}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Alex Martin Loewi \\
  \texttt{aloewi@cmu.edu}
  \And
  Francisco Ralston Fonseca\\
  \texttt{fralston@andrew.cmu.edu}
  \And
  Octavio Mesner\\
  \texttt{omesner@cmu.edu}
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

%Testing citations \cite{10.2307/2290079}, \cite{kang2007,RSSB:RSSB12027}, \cite{Hoff2002, gollini-2016, salter-townshend2017}.

\subsection{The Curious Case of SNA}
Social network analysis (SNA) is a complex, and inherently interdisciplinary field. It takes the already complex fields of psychology and sociology, and focuses on the most interdependent processes in these disciplines. This then creates systems so complicated that sophisticated statistical models are necessary to make any sense of them. While this complexity makes SNA an exciting intersection, effectively spanning these two disparate approaches is exceptionally difficult. As a result, while it attracts a great deal of attention, many important problems in SNA are under-studied, simply because of the small number of people who are trained to deal with both the substance, and methods, of the field.

One of these problems is that of modeling multigraphs. Multigraphs, also called multiview networks, can be thought of as either a single graph with multiple edge types, or as multiple networks over the same set of nodes. Data of this kind is not uncommon, at either large or small scales. Twitter has Following relationships, as well as Retweets, and Likes, each of which can be thought of as simply one layer within the full relationship between two nodes. In small scales, surveys for networks very rarely ask about a single type of relationship, and many of the most famous network data sets have multiple relationships (such as Trust, Friendship, and Respect, in Samson's monk data). Despite the existence of data of this form for decades, the strange nature and inherent difficulty of this data has led to an extremely small number of ways to model it, all of which have serious shortcomings. The models from statistics tend to ignore the problems of the practitioner, and the practitioners rarely have the training to develop their own methods.

\subsection{The Practical Difficulties of Multigraphs}
In particular, practitioners have two immediate problems when dealing with multigraphs, both of which are consequences of the inherent complexity of the data. 
\subsubsection{Visualization}
The first problem is that multigraphs are too complex to easily visualize, and visualization has always been a cornerstone of network analysis. In particular, the difficulty comes in comparing the many layers of data (i.e. the graphs formed by the different edge types). Because two layers have the same set of nodes, comparing them requires the nodes to be in similar positions for the two layouts. However, graph layout algorithms are designed to pay attention only to the connectivity in a graph, and find the node positions that best reflect that connectivity -- if the node positions are fixed, all of the meaningful structure in a layer may be obscured. To define the problems in terms of its extremes, there is a trade-off in terms of comparability (fixed nodes) and expressiveness (free nodes). A useful multigraph model would be able to tune explicitly between these two extremes.

\subsubsection{Dimensionality}
The second problem is that multigraphs may be unnecessarily complex. Empirically, there are often high degrees of correlation between the layers in a multigraph, and the smaller the number of graphs, the easier analysis becomes. These two observations combine to suggest another useful property of a new model -- the ability to remove redundant layers. While existing models often take the data and make it even more complicated, that runs counter to the needs of an applied social network analyst.

\subsection{Previous Work}
\cite{Amer2017, Friedman2008, Friedman2010, Gollini2017, Qin, Salter-townshend2013, Vincent, Wu2008}

\section{The Model}
\subsection{Problem Statement}
Motivated by these problems, we propose a model with the following three objectives:

\begin{enumerate}
\item Use current methods from statistical SNA to model multigraphs
\item Model the layers of the multigraph in a way that can tune for comparability
\item Allow the model to remove redundant layers
\end{enumerate}

A model with all of these qualities, which we term the Hierarchical Latent Space Model, can be described in the following way:

\subsection{The Likelihood Function}
Using the canonical Latent Space Model \cite{Hoff2002} as a starting point, we model a set of conditionally independent binary dyads. 

The existence of each of these dyads is a function of an intercept, a set of covariates (omitted here for clarity), and the distance between the latent ``position'' variables $z$ of the two nodes in the dyad. The goal of the model will be to estimate these positions. An optimal set of variables would place positions close together for nodes with an observed edge, and vice versa.

\[
\eta_{ijk} = \alpha_k - \|z_{ik} - z_{jk}\|_2^2 %=  \alpha_k - \|b_i + \epsilon_{ik} - b_j - \epsilon_{jk}\|^2
\]

The indices $i$ and $j$ are over the nodes; the index $k$ refers to the different layers in the multigraph. Because the edges are binary, we use the inverse logit $\sigma$ to transform $\eta$, but it should be observed that for real-valued edges, other link functions could be easily substituted.

\[
\sigma(\eta_{ijk}) = \frac{1}{1+\exp(-\eta_{ijk})} \in [0, 1]
\]

All together, the unpenalized likelihood of the HLSM thus takes the form of a binomial:

\begin{align*}
  L &= \prod_k \prod_{i}\prod_{j<i} \sigma(\eta_{ijk})^{y_{ijk}}(1-\sigma(\eta_{ijk}))^{1-y_{ijk}}
\end{align*}

and the log likelihood is

\begin{align*}
  \ell 	&= \sum_k \sum_{i} \sum_{j < i} {y_{ijk}} \ln \sigma(\eta_{ijk}) + (1-y_{ijk})\ln(1- \sigma(\eta_{ijk})) \\
  	&= \sum_k \sum_{i} \sum_{j < i} - {y_{ijk}} \ln (1+\exp(-\eta_{ijk})) + (1-y_{ijk})\ln\left (\frac{\exp(-\eta_{ijk})}{1+\exp(-\eta_{ijk})}\right) \\
  	&= \sum_k \sum_{i} \sum_{j < i} - {y_{ijk}} \ln (1+\exp(-\eta_{ijk})) + (1-y_{ijk})[-\eta_{ijk} - \ln (1+\exp(-\eta_{ijk}))] \\
  	&=\sum_k \sum_{i} \sum_{j < i} - {y_{ijk}} \ln (1+\exp(-\eta_{ijk})) -\eta_{ijk} - \ln (1+\exp(-\eta_{ijk})) + y_{ijk} \eta_{ijk} + y_{ijk}\ln (1+\exp(-\eta_{ijk}))] \\
 	& = \sum_k \sum_{i} \sum_{j < i} (y_{ijk}-1)\eta_{ijk} - \ln (1+\exp(-\eta_{ijk}))
\end{align*}

\subsection{Regularization}

While the notation $z$ for the latent variables is consistent with the literature, and somewhat more intuitive, our contribution comes from a reparameterization of the model, where

\[
z_{ik} = b_i + \epsilon_{ik}
\]

This parameterization takes an explicit mid-point between the two most recently published papers on this problem. One \cite{salter-townsend} fits the layers independently, and allows correlations between them; another \cite{} treats all of the layers simultaneously, as part of a single complex object. Our model allows not only for similar behaviors to both of these extremes, but additionally allows the user to tune between them manually.

Our model starts with a ``base'' position $b_i$ for each node, which is the hierarchical layer in the model. It then adds a layer-specific perturbation $\epsilon_{ik}$. The behavior of the model then depends entirely on the regularization placed on the $\epsilon$'s. When there is none, each layer is fit independently. When there is an arbitrarily large amount, the perturbations are all driven to zero, and all $k$ layers are represented identically by the base parameters $b$. With intermediate values however, the user can find the point between these two extremes that allows for the graphs to be distinct, but also renders them sufficiently similar to be comparable, by not allowing the perturbations to stray too far from their shared base position.

Furthermore, with the use of a group lasso penalty applied to all of the perturbations within a single layer $k$, the model itself will perform graph-wise dimensionality reduction. This is far more valuable to the practitioner than an element-wise sparse solution, which would still require them to consider all of the many layers in their multigraph. %This leads to the following penalized negative log likelihood, that we will attempt to optimize: 

%\begin{align*}
% 	\ell = - \sum_k \sum_{i} \sum_{j < i} (y_{ijk}-1)\eta_{ijk} + \ln (1+\exp(-\eta_{ijk})) + \lambda \sum_k \|\epsilon_k\|_2
%\end{align*}

%\subsection{Derivations}


\section{Fitting}
\subsection{The Optimization Problem}

Together these two elements form our optimization problem, which is to minimize the sum of the negative log likelihood and a lasso penalty on the deviations $\epsilon_{ik}$.

\begin{align*}
\min_{\epsilon, b} \sum_k \sum_{i} \sum_{j < i} [ (1 - y_{ijk})\eta_{ijk} +  \ln (1+\exp(\eta_{ijk})) ] + \lambda \sum_k \| \epsilon_{k}\|_2
\end{align*}

where $\epsilon_k$ represents all of the perturbations for layer $k$. Since this objective is not differentiable, we have several options, but the clear first choice is to use proximal gradient descent. %If the gradients for the ML function derived above are correct, we can directly use the code we implemented in HW3 (problem 4) to implement the proximal operator for this problem.

\subsection{Derivation of the Gradients}

We define our decision variables as: $\{b_i\}$ $\forall~i$ and $\{\epsilon_{ik}\}$ $\forall~i,k$.

Then, the gradients of the $\eta_{ijk}$ are:

\[
\frac{\partial \eta_{ijk}}{\partial b_{i}} =  - 2(b_i + \epsilon_{ik} - b_j - \epsilon_{jk})
\]

\[
\frac{\partial \eta_{ijk}}{\partial b_{j}} = 2(b_i + \epsilon_{ik} - b_j - \epsilon_{jk})
\]

\[
\frac{\partial \eta_{ijk}}{\partial \epsilon_{ik}} = - 2(b_i + \epsilon_{ik} - b_j - \epsilon_{jk})
\]

\[
\frac{\partial \eta_{ijk}}{\partial \epsilon_{jk}} = 2(b_i + \epsilon_{ik} - b_j - \epsilon_{jk})
\]

\begin{align*}
\frac{\partial \ell}{\partial b_{m}} & =  \sum_k \sum_{i}  \sum_{j<i} (y_{ijk} -1)\frac{\partial \eta_{ijk}}{\partial b_{m}} - \frac{\partial \ln (1+\exp(-\eta_{ijk}))}{\partial b_{m}} \\
& =  \sum_k\left[ \sum_{j<m} (y_{mjk} -1)\frac{\partial \eta_{mjk}}{\partial b_{m}} - \frac{\partial \ln (1+\exp(-\eta_{mjk}))}{\partial b_{m}} +  \sum_{i > m} (y_{imk} -1)\frac{\partial \eta_{imk}}{\partial b_{m}} - \frac{\partial \ln (1+\exp(-\eta_{imk}))}{\partial b_{m}} \right] \\
%
& =  \sum_k\bigg[ -2 \sum_{j<m} \left[(y_{mjk} -1)(b_m + \epsilon_{mk} - b_j + \epsilon_{jk}) + \frac{\exp(-\eta_{mjk})}{1+\exp(-\eta_{mjk})}(b_m + \epsilon_{mk} - b_j - \epsilon_{jk}) \right] \\
& \quad\quad\quad +  2 \sum_{i > m} \left[(y_{imk} -1)(b_i + \epsilon_{ik} - b_m - \epsilon_{mk}) +\frac{\exp(-\eta_{ijk})}{1+\exp(-\eta_{mjk})}(b_i + \epsilon_{ik} - b_m - \epsilon_{mk}) \right] \bigg] \\
& =  2 \sum_k\bigg[ - \sum_{j<m} \left(y_{mjk} -1 + \frac{\exp(-\eta_{mjk})}{1+\exp(-\eta_{mjk})}\right)(z_{mk} - z_{jk})\\
& \quad\quad\quad\quad +  \sum_{i > m} \left(y_{imk} -1 + \frac{\exp(-\eta_{ijk})}{1+\exp(-\eta_{mjk})}\right)(z_{ik} - z_{mk}) \bigg] \\
%
\end{align*}

\begin{align*}
\frac{\partial \ell}{\partial \epsilon_{mk}} & = 2 \bigg[ - \sum_{j<m} \left(y_{mjk} -1 + \frac{\exp(-\eta_{mjk})}{1+\exp(-\eta_{mjk})}\right)(z_{mk} - z_{jk})\\
& \quad\quad +  \sum_{i > m} \left(y_{imk} -1 + \frac{\exp(-\eta_{ijk})}{1+\exp(-\eta_{mjk})}\right)(z_{ik} - z_{mk}) \bigg]
\end{align*}

\subsection{The Proximal Operator}

The proximal operator for the penalty term using the L2 norm\footnotemark~ is:

\footnotetext{Using the results from Homework 3}

\[
\text{prox}_{\|.\|, t}(\epsilon_{i,k}) = 
\begin{cases}
\frac{\|\epsilon_{i,k}\| - \lambda t}{\|\epsilon_{i,k}\|}\epsilon_{i,k}, & \|\epsilon_{i,k}\| \ge \lambda t \\
0, & \|\epsilon_{i,k}\| < \lambda t
\end{cases}
\]

\[
\text{prox}_{\|.\|, t}(b_i) = b_i
\]

Therefore, let $\beta$ be the vector with all parameters $\epsilon$ and $b$. The Proximal Gradient Descent method in this case is:

\[
\beta^{+} = \text{prox}_{\|.\|, t}(\beta - t \nabla \ell(\beta))
\]

\subsection{Non-Convexity and Initialization}
The original Latent Space Model is known to be non-convex in the latent positions $z$ \cite{Hoff2002}, and our model is no different. This issue is typically approached by using MCMC samplers \cite{Hoff2002, Carpenter2017} which are capable of exploring non-convex spaces, however these are known to have issues as well. Using them on this problem proved not only to take far more time, but also to have substantially worse results. We restrict our discussion to the non-Bayesian approaches taken.

As we use methods designed for convex problems, we start by using a careful initialization of our procedure. This attempts to begin the algorithm close to a global minimum of the objective, so that when we do descend the likelihood, we descend a good one, knowing that local minima exist. This initialization was composed of several steps.
\\
\begin{enumerate}
\item Create a proposal `base' graph from $\hat{y}_{ij} =\sum_k y_{ijk} > 0$
\item Fit a separate single-graph latent space model to each layer
\item Transform the layer estimates to minimize their distance from the base estimate
\end{enumerate}

The third step in this procedure was initially done with a Procrustes transform \cite{Hoff2002}, as is used in Bayesian estimation of Latent Space Models. This transform is used because proposal graphs in a sampling procedure can produce a graph with many different unimportant variations, as the likelihood is invariant to translation, rotation, scaling, and flipping. The Procrustes transform is thus used to remove these transformations, while keeping the variations that are of importance to the likelihood. However this transform minimizes the overall distance between \textit{all} pairs of points in two shapes, and in this problem, we require something slightly different. The goal here is to minimize only distances between positions that correspond to the same node -- this is the property that minimizes the $\epsilon$'s, and makes the embeddings comparable.

To solve this problem we developed and propose the Anticrustean transformation. A closed form solution, should one exist, has not been found, but a simple implementation combines line searches over the several classes of transforms to which the likelihood is invariant. Together, these steps produce a good initial estimate of all of the models parameters, which are used as starting points in the proximal gradient procedure. 

%\begin{algorithmic}
%\If {$i\geq maxval$}
%    \State $i\gets 0$
%\Else
%    \If {$i+k\leq maxval$}
%        \State $i\gets i+k$
%    \EndIf
%\EndIf
%\end{algorithmic}



\section{Results}

\lipsum[4]


\section{Discussion}




\section{Conclusion}

Despite not being a convex problem, a combination of convex methods and a good heuristic starting point have been shown to efficiently fit this novel class of models. In addition, they have demonstrated all of the properties for which they were intended, and several others additionally. As demonstrated, HLSMs are capable of producing comparable graph layouts for the layers of a multigraph, and are also capable of performing layer-wise dimensionality reduction in order to meaningfully simplify this complex form of data.

They are also valuable for refining the layouts determined by current Latent Space Model implementations, as can been seen in the differences between the initializations, and the final estimates. While it is possible that many likelihoods are feasible from a numerical point of view, this further convex optimization is valuable in finding solutions that are meaningful to human interpreters. Furthermore, our fitting procedure was found to converge to a good solution far faster than the HMC implementation with which we compared it.

Having demonstrated the basic feasibility, effectiveness, and utility, of HLSMs, the next steps will be to push its limits on different kinds of graphs, and different numbers of layers. Several different guests at our poster session mentioned additional problems, including in medical domains, and the interpretability of deep neural nets, to which our method might be able to contribute. We look forward to exploring these additional problem spaces.

\bibliography{hlsm.bib}

\end{document}
