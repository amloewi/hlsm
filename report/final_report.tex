\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\PassOptionsToPackage{numbers, sort}{natbib}
\usepackage[final]{nips_2017}
\bibliographystyle{unsrtnat}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{lipsum}

\usepackage{amssymb, amsmath}

\title{Hierarchical Latent Space Models}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Alex Martin Loewi \\
  \texttt{aloewi@cmu.edu}
  \And
  Francisco Ralston Fonseca\\
  \texttt{fralston@andrew.cmu.edu}
  \And
  Octavio Mesner\\
  \texttt{omesner@cmu.edu}
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  abstract...
\end{abstract}

\section{Introduction}

Testing citations \cite{10.2307/2290079}, \cite{kang2007,RSSB:RSSB12027}, \cite{hoff-2002, gollini-2016, salter-townshend2017}.


Social network analysis (SNA) is a very strange field, in a number of ways. it is inherently interdisciplinary, even its name indicating that it emphasizes both sophisticated statistical machinery, and also much more qualitative elements. While this is an exciting intersection, spanning both of these such disparate approaches is exceptionally difficult. As a result many important problems in SNA are under-studied, simply because of the small number of people who think about the problems it considers, in their full depth.

One of these problems is that of modeling multigraphs. Multigraphs, also called multiview networks, can be thought of as either a single graph with multiple edge types, or as multiple networks over the same set of nodes. Data of this kind is not uncommon, at either large or small scales. Twitter has Following relationships, as well as Retweets, and Likes, each of which can be thought of as simply one layer within the full relationship between two nodes. In small scales, surveys for networks very rarely ask about a single type of relationship, and many of the most famous network data sets have multiple relationships (such as Trust, Friendship, and Respect, in Samson's monk data). Despite the existence of data of this form for decades, the strange nature and inherent difficulty of this data has led to an extremely small number of ways to model it.

In designing a model however, the complexity of the data requires a perspective on what aspect of the network is most important. The model considered here was inspired initially by a very practical problem. A particular data set had five layers, and the goal was to compare visualizations of the five separate layers, to gain an intuition for similarities in the networks (visualization has long been a cornerstone of SNA, and graph layout algorithms is a substantial field unto itself). However, even if it has the same set of nodes, a graph with different sets of edges may be laid out completely differently -- incomparably -- from a graph with a different set of edges. The other approach, to immediately solve this problem of incomparable node placement, is to fix all of the positions a priori. However it has also long been known that doing so is the least informative visualization possible, as the very point of layout algorithms is to drive placement as a function of connectivity. This inspired the thought that perhaps a model could trade off these two extremes -- allowing just enough flexibility that the layout of a single layer could be both informative, and also compared meaningfully with the other layers in a graph.

\section{Problem Statement}

\subsection{The Likelihood function}

The Binomial likelihood equation of the HLSM is:

\begin{align*}
  L &= \prod_k \prod_{i}\prod_{j<i} \sigma(\eta_{ijk})^{y_{ijk}}(1-\sigma(\eta_{ijk}))^{1-y_{ijk}}
\end{align*}

where,

\[
\eta_{ijk} = \alpha_k - \|z_{ik} - z_{jk}\|^2 =  \alpha_k - \|b_i + \epsilon_{ik} - b_j - \epsilon_{jk}\|^2
\]

\[
z_{ik} = b_i + \epsilon_{ik}
\]

\[
\sigma(\eta_{ijk}) = \frac{1}{1+\exp(-\eta_{ijk})}
\]

Then, the log likelihood is

\begin{align*}
  \ell 	&= \sum_k \sum_{i} \sum_{j < i} {y_{ijk}} \ln \sigma(\eta_{ijk}) + (1-y_{ijk})\ln(1- \sigma(\eta_{ijk})) \\
  	&= \sum_k \sum_{i} \sum_{j < i} - {y_{ijk}} \ln (1+\exp(-\eta_{ijk})) + (1-y_{ijk})\ln\left (\frac{\exp(-\eta_{ijk})}{1+\exp(-\eta_{ijk})}\right) \\
  	&= \sum_k \sum_{i} \sum_{j < i} - {y_{ijk}} \ln (1+\exp(-\eta_{ijk})) + (1-y_{ijk})[-\eta_{ijk} - \ln (1+\exp(-\eta_{ijk}))] \\
  	&=\sum_k \sum_{i} \sum_{j < i} - {y_{ijk}} \ln (1+\exp(-\eta_{ijk})) -\eta_{ijk} - \ln (1+\exp(-\eta_{ijk})) + y_{ijk} \eta_{ijk} + y_{ijk}\ln (1+\exp(-\eta_{ijk}))] \\
 	& = \sum_k \sum_{i} \sum_{j < i} (y_{ijk}-1)\eta_{ijk} - \ln (1+\exp(-\eta_{ijk}))
\end{align*}

\subsection{Optimization Problem}

Finally, our complete objective function is to minimize the sum between the negative log likelihood and a lasso penalty on the deviations $\epsilon_{ik}$.

\begin{align*}
\min_{\epsilon, b} \sum_k \sum_{i} \sum_{j < i} [ (1 - y_{ijk})\eta_{ijk} +  \ln (1+\exp(\eta_{ijk})) ] + \lambda \sum_k \sum_i \| \epsilon_{i,k}\|
\end{align*}

where $\|.\|$ represents the L2 norm. Since this is not differentiable, we could use proximal gradient descent. If the gradients for the ML function derived above are correct, we can directly use the code we implemented in HW3 (problem 4) to implement the proximal operator for this problem.

\subsection{Derivation of the gradients}

We define our decision variables as: $\{b_i\}$ $\forall~i$ and $\{\epsilon_{ik}\}$ $\forall~i,k$.

Then, the gradients of the $\eta_{ijk}$ are:

\[
\frac{\partial \eta_{ijk}}{\partial b_{i}} =  - 2(b_i + \epsilon_{ik} - b_j - \epsilon_{jk})
\]

\[
\frac{\partial \eta_{ijk}}{\partial b_{j}} = 2(b_i + \epsilon_{ik} - b_j - \epsilon_{jk})
\]

\[
\frac{\partial \eta_{ijk}}{\partial \epsilon_{ik}} = - 2(b_i + \epsilon_{ik} - b_j - \epsilon_{jk})
\]

\[
\frac{\partial \eta_{ijk}}{\partial \epsilon_{jk}} = 2(b_i + \epsilon_{ik} - b_j - \epsilon_{jk})
\]

\begin{align*}
\frac{\partial \ell}{\partial b_{m}} & =  \sum_k \sum_{i}  \sum_{j<i} (y_{ijk} -1)\frac{\partial \eta_{ijk}}{\partial b_{m}} - \frac{\partial \ln (1+\exp(-\eta_{ijk}))}{\partial b_{m}} \\
& =  \sum_k\left[ \sum_{j<m} (y_{mjk} -1)\frac{\partial \eta_{mjk}}{\partial b_{m}} - \frac{\partial \ln (1+\exp(-\eta_{mjk}))}{\partial b_{m}} +  \sum_{i > m} (y_{imk} -1)\frac{\partial \eta_{imk}}{\partial b_{m}} - \frac{\partial \ln (1+\exp(-\eta_{imk}))}{\partial b_{m}} \right] \\
%
& =  \sum_k\bigg[ -2 \sum_{j<m} \left[(y_{mjk} -1)(b_m + \epsilon_{mk} - b_j + \epsilon_{jk}) + \frac{\exp(-\eta_{mjk})}{1+\exp(-\eta_{mjk})}(b_m + \epsilon_{mk} - b_j - \epsilon_{jk}) \right] \\
& \quad\quad\quad +  2 \sum_{i > m} \left[(y_{imk} -1)(b_i + \epsilon_{ik} - b_m - \epsilon_{mk}) +\frac{\exp(-\eta_{ijk})}{1+\exp(-\eta_{mjk})}(b_i + \epsilon_{ik} - b_m - \epsilon_{mk}) \right] \bigg] \\
& =  2 \sum_k\bigg[ - \sum_{j<m} \left(y_{mjk} -1 + \frac{\exp(-\eta_{mjk})}{1+\exp(-\eta_{mjk})}\right)(z_{mk} - z_{jk})\\
& \quad\quad\quad\quad +  \sum_{i > m} \left(y_{imk} -1 + \frac{\exp(-\eta_{ijk})}{1+\exp(-\eta_{mjk})}\right)(z_{ik} - z_{mk}) \bigg] \\
%
\end{align*}

\begin{align*}
\frac{\partial \ell}{\partial \epsilon_{mk}} & = 2 \bigg[ - \sum_{j<m} \left(y_{mjk} -1 + \frac{\exp(-\eta_{mjk})}{1+\exp(-\eta_{mjk})}\right)(z_{mk} - z_{jk})\\
& \quad\quad +  \sum_{i > m} \left(y_{imk} -1 + \frac{\exp(-\eta_{ijk})}{1+\exp(-\eta_{mjk})}\right)(z_{ik} - z_{mk}) \bigg]
\end{align*}

\subsection{The proximal operator}

The proximal operator for the penalty term using the L2 norm\footnotemark~ is:

\footnotetext{According to the results on Homework 3}

\[
\text{prox}_{\|.\|, t}(\epsilon_{i,k}) = 
\begin{cases}
\frac{\|\epsilon_{i,k}\| - \lambda t}{\|\epsilon_{i,k}\|}\epsilon_{i,k}, & \|\epsilon_{i,k}\| \ge \lambda t \\
0, & \|\epsilon_{i,k}\| < \lambda t
\end{cases}
\]

\[
\text{prox}_{\|.\|, t}(b_i) = b_i
\]

Therefore, let $\beta$ be the vector with all parameters $\epsilon$ and $b$. The Proximal Gradient Descent method in this case is:

\[
\beta^{+} = \text{prox}_{\|.\|, t}(\beta - t \nabla \ell(\beta))
\]

\section{Results}

\lipsum[4]


\section{Discussion}

\lipsum[5]

\section{Conclusion}

\lipsum[6]

\bibliography{sample.bib}

\end{document}
